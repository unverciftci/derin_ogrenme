<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Softmaks Regresyonu (✗) | Derin Öğrenme Kitabı</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Softmaks Regresyonu (✗)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Derin Öğrenmeye Giriş" />
<meta property="og:description" content="Derin Öğrenmeye Giriş" />
<link rel="canonical" href="https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html" />
<meta property="og:url" content="https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html" />
<meta property="og:site_name" content="Derin Öğrenme Kitabı" />
<meta property="og:image" content="https://unverciftci.github.io/derin_ogrenme/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-21T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Derin Öğrenmeye Giriş","mainEntityOfPage":{"@type":"WebPage","@id":"https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html"},"@type":"BlogPosting","url":"https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html","headline":"Softmaks Regresyonu (✗)","dateModified":"2020-02-21T00:00:00-06:00","datePublished":"2020-02-21T00:00:00-06:00","image":"https://unverciftci.github.io/derin_ogrenme/images/chart-preview.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/derin_ogrenme/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://unverciftci.github.io/derin_ogrenme/feed.xml" title="Derin Öğrenme Kitabı" /><link rel="shortcut icon" type="image/x-icon" href="/derin_ogrenme/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Softmaks Regresyonu (✗) | Derin Öğrenme Kitabı</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Softmaks Regresyonu (✗)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Derin Öğrenmeye Giriş" />
<meta property="og:description" content="Derin Öğrenmeye Giriş" />
<link rel="canonical" href="https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html" />
<meta property="og:url" content="https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html" />
<meta property="og:site_name" content="Derin Öğrenme Kitabı" />
<meta property="og:image" content="https://unverciftci.github.io/derin_ogrenme/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-21T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Derin Öğrenmeye Giriş","mainEntityOfPage":{"@type":"WebPage","@id":"https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html"},"@type":"BlogPosting","url":"https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html","headline":"Softmaks Regresyonu (✗)","dateModified":"2020-02-21T00:00:00-06:00","datePublished":"2020-02-21T00:00:00-06:00","image":"https://unverciftci.github.io/derin_ogrenme/images/chart-preview.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://unverciftci.github.io/derin_ogrenme/feed.xml" title="Derin Öğrenme Kitabı" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper">
<a class="site-title" rel="author" href="/derin_ogrenme/">Derin Öğrenme Kitabı</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/derin_ogrenme/bilgi/">Bilgi</a><a class="page-link" href="/derin_ogrenme/arama/">Arama</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Softmaks Regresyonu (✗)</h1>
<p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-02-21T00:00:00-06:00" itemprop="datePublished">
        Feb 21, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      14 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/unverciftci/derin_ogrenme/tree/master/_notebooks/2020-02-21-Linear-Siniflandirma.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/derin_ogrenme/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/unverciftci/derin_ogrenme/master?filepath=_notebooks%2F2020-02-21-Linear-Siniflandirma.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/derin_ogrenme/assets/badges/binder.svg" alt="Open In Binder">
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/unverciftci/derin_ogrenme/blob/master/_notebooks/2020-02-21-Linear-Siniflandirma.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/derin_ogrenme/assets/badges/colab.svg" alt="Open In Colab">
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1">
<a href="#Softmaks-Regresyonu">Softmaks Regresyonu </a>
<ul>
<li class="toc-entry toc-h2">
<a href="#S%C4%B1n%C4%B1fland%C4%B1rma-(Classification)">Sınıflandırma (Classification) </a>
<ul>
<li class="toc-entry toc-h3"><a href="#A%C4%9F-Yap%C4%B1s%C4%B1-(Network-Architecture)">Ağ Yapısı (Network Architecture) </a></li>
<li class="toc-entry toc-h3"><a href="#Softmaks-%C4%B0%C5%9Flemi-(Softmax-Operation)">Softmaks İşlemi (Softmax Operation) </a></li>
<li class="toc-entry toc-h3"><a href="#Vectorization-for-Minibatches">Vectorization for Minibatches </a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#Loss-Function">Loss Function </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Log-Likelihood">Log-Likelihood </a></li>
<li class="toc-entry toc-h3"><a href="#Softmax-and-Derivatives">Softmax and Derivatives </a></li>
<li class="toc-entry toc-h3"><a href="#Cross-Entropy-Loss">Cross-Entropy Loss </a></li>
</ul>
</li>
<li class="toc-entry toc-h2">
<a href="#Information-Theory-Basics">Information Theory Basics </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Entropy">Entropy </a></li>
<li class="toc-entry toc-h3"><a href="#Surprisal">Surprisal </a></li>
<li class="toc-entry toc-h3"><a href="#Cross-Entropy-Revisited">Cross-Entropy Revisited </a></li>
<li class="toc-entry toc-h3"><a href="#Kullback-Leibler-Divergence">Kullback-Leibler Divergence </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Model-Prediction-and-Evaluation">Model Prediction and Evaluation </a></li>
<li class="toc-entry toc-h2"><a href="#Summary">Summary </a></li>
<li class="toc-entry toc-h2"><a href="#Exercises">Exercises </a></li>
</ul>
</li>
</ul>
<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-02-21-Linear-Siniflandirma.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Softmaks-Regresyonu">
<a class="anchor" href="#Softmaks-Regresyonu" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmaks Regresyonu<a class="anchor-link" href="#Softmaks-Regresyonu"> </a>
</h1>
<p>Önceki bölümlerde lineer regresyon tanıtıldı, daha sonra nasıl hesaplanacağını hatta büyük veri olması durumunda özel kütüphaneler ile nasıl iş yapılacağını gördük.</p>
<p>Regresyon, bir şeyin ne kadar, kaç tane olduğu gibi sorulara cevap aradığımızda kullandığımız bir araçtır. Bir evin kaç araya satılabileceğini, bir takımın kaç maç kazanabileceğini veya bir hastanın hastanede kaç gün yatacağını tahimin etmek istiyorsanız, muhtemelen regresyon sizin için en iyisidir.</p>
<p>Pratik uygulamalarda sınıflandırma daha çok gereklidir. Sınıflandırmada nesnelerin <em>ne kadar</em> olduğu yerine <em>hangisi</em> olduğu ile ilgileniriz:</p>
<ul>
<li>Bu e-posta istenmeyen bildirim midir?</li>
<li>Bu müşterinin hizmet alması muhtemel midir?</li>
<li>Bu resim maymun, köpek, kedi veya horoz arasından hangisinin resmidir?</li>
<li>Hangi film sayfayı ziyaret eden kişinin en çok izlemek isteyeceği türdendir?</li>
</ul>
<p>Makine öğrenmesi uygulayıcıları günlük kullanımda <em>sınıflandırma</em> ifadesini birbirinden ince bir çizgiyle ayrılan iki durum için kullanılır: (i) Örneğin hangi sınıfa ait olduğunu <em>kesin</em> olarak belirlemek (hard assignment) istediğimiz durum. (ii) <em>Esnek</em> olarak belirlemek (soft assignment), yani her bir sınıfa ait olma <em>olasığının</em> belirlenmesi. Aradaki fark detaya indiğinizde kapanır çünkü kesin olarak sınıfları belirlerken de aslında olasılık kullanırız.</p>
<h2 id="Sınıflandırma-(Classification)">
<a class="anchor" href="#S%C4%B1n%C4%B1fland%C4%B1rma-(Classification)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sınıflandırma (Classification)<a class="anchor-link" href="#S%C4%B1n%C4%B1fland%C4%B1rma-(Classification)"> </a>
</h2>
<p>Konuya çabuk ısınmak adına, $2\times2$ boyutunda siyah-beyaz resimlerin sınıflandırılması örneği ile başlayalım. Her bir pikseli bir sayı ile gösterirsek, bir resim için $x_1, x_2, x_3, x_4$ ile gösterdiğimiz dört özellik belirlenmiş olur. Dahası, resimlerin "kedi", "tavuk" ve "köpek" sınıflarından birine ait olduğunu kabul edelim.</p>
<p>Şimdi etiketlere yani resmin ait olduğu sınıfı nasıl ifade edeceğimize karar vermeliyiz. En doğal yol {kedi, tavuk, köpek} sınıflarını, birden üçe kadar olan tamsayılarla gösterip $y \in \{1, 2, 3\}$ biçiminde ifade etmektedir. BU şekilde bilgisayarın hafızasına yük olmayız. Sınıflar arasıda doğal bir sıralama söz konusu olsaydı problemimiz regresyon problemi olurdu ve bu tür etiketleri kullannabilirdik. Mesela {bebek, çokuk, genç, yetişkin, yaşlı} örneğinde olduğu gibi.</p>
<p>Ancak genellikle sınıflandırma problemlerinde sınıflar arasında doğal bir hiyerarşi yoktur. Neyse ki İstatistikçiler uzun yıllar önce buna bir çözüm buldular: <em>tek aktif kod</em> (one-hot encoding). Tek aktif kod, sınıf sayısı kadar bileşene sahip bir vektördür. Veri kümseindeki bir örnek için tek aktif kod vektörü oluşturulurken, örneğin ait olduğu bileşen $1$, diğer bilşenler $0$ alı alınır. Bu durumda yukarıdaki üçlü sınıflandırmada etiketler</p>
<p>
$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$
</p>
<p>şeklinde vektörler olur. Eğer $y$ vektörü $(1, 0, 0)$ ise "kedi", $(0, 1, 0)$ ise "tavuk" ve $(0, 0, 1)$ ise "köpek" belirtir.</p>
<h3 id="Ağ-Yapısı-(Network-Architecture)">
<a class="anchor" href="#A%C4%9F-Yap%C4%B1s%C4%B1-(Network-Architecture)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ağ Yapısı (Network Architecture)<a class="anchor-link" href="#A%C4%9F-Yap%C4%B1s%C4%B1-(Network-Architecture)"> </a>
</h3>
<p>Her sınıfın koşullu olasılığını tahmin etmek için çoklu çıktı veren bir modele ihtiyacımız var. Lineer modellerle sınıflandırma yapabilmek amacıyla çıktı sayısı kadar lineer fonksiyon gerekir. Her çıktı kendi lineer fonksiyonuna karşılık gelir. Bizim durumumuzda $3$ çıktı sınıfı ve $4$ özellik olduğundan, $w$ ile göstereceğimiz $12$ tane ağırlık (weight), $b$ ile göstereceğimiz $3$ tane yanlılık (bias) terimleri olacaktır. Şimdi $o$ ile gösterilen her çıktı (output) için</p>
$$
\begin{aligned}
o_1 &amp;= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &amp;= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &amp;= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3
\end{aligned}
$$<p>elde edilir. Bu hesabı aşağıdaki sinir ağı (neural network) diyagramı ile gösterebiliriz. Lineer regresyonda olduğu gibi, lojistik regresyon modeli de bir tek katman içerir. Her bir $o_1, o_2$, and $o_3$ çıktısı tüm $x_1$, $x_2$, $x_3$, and $x_4$ girdileri ile bağımlı olduğundan, çıktı katmanı tam bağlantılı katmanıdır (fully-connected layer).</p>
<p><img src="https://d2l.ai/_images/softmaxreg.svg" alt="Softmax regression is a single-layer neural network.  "></p>
<p>Modelimizi daha derlitoplu ifade etmek için, Lineer Cebir gösterimi kullanacağız. Vektör formatında $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$ yazarak hem matematiksel açıdan hem de kod yazmada daha uygun bir form elde edilir. Burada tüm ağırlıklar $3\times4$ ebatında $\mathbf{W}$ matrisini oluşturur. Verilen bir $\mathbf{x}$veri örneği için çıktı vektörümüz, ağırlık matris ile örnek vektörünün çarpımına $\mathbf{b}$ yanlılık vektörünün eklenmesiyle elde edilir.</p>
<h3 id="Softmaks-İşlemi-(Softmax-Operation)">
<a class="anchor" href="#Softmaks-%C4%B0%C5%9Flemi-(Softmax-Operation)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmaks İşlemi (Softmax Operation)<a class="anchor-link" href="#Softmaks-%C4%B0%C5%9Flemi-(Softmax-Operation)"> </a>
</h3>
<p>Modelimizin çıktılarını olasılıklar olarak yorumlamak istiyoruz. Parametrelerimizi, verinin olabılırılığini maksimum yapacak şekilde belirleyeceğiz. Daha sonra da tahminler üretebilmek için, bir eşik belirleyeceğiz. Örneğin elde edilen olasılıklar üzerinden en büyüğüne karşılık gelen sınıfı seçeceğiz.</p>
<p>Daha açık ifade edelim: $\hat{y}_k$ çıktısını verilen bir girdinin $k$ sınıfında olma olasılığı anlamını taşımasını istiyoruz. Bu durumda $\operatorname*{argmax}_k y_k$ işlemi sonucunda en büyük değere sahip sınıfı seçeriz. Mesela $\hat{y}_1$, $\hat{y}_2$, and $\hat{y}_3$ çıktıları $0.1$, $0.8$, and $0.1$ değerlerini alıyorsa, seçeceğimiz sınıf $2$ numaralı sınıftır, yani örneğimizdeki "tavuk" resmidir.</p>
<p>Yukarıda ${o}$ ile gösterilen ve "logit" denilen sayılar modelin çıktıları gibi görünebilir. Ancak lineer katmanın çıktılarını olasılıklar gibi ele almak sorun çıkarabilir. Bunların toplamının $1$ olması gerekiyor. Hatta girdilere bağlı olarak negatif değerler bile alıyorlar. Bunlar olasılığın temel aksiyomlarını (kabullerini) ihlal ediyor.</p>
<p>Bunları olasılıklarolarak alabilmemiz için her girdi için toplamlarının $1$ olması gerekiyor. Dahası kayıp fonksiyonunu da olasılıkların anlamlı olmasını sağlayacak şekilde seçmeliyiz. Modelimizin $0.5$ sonucunu verdiği tüm durumlarda bu örneklerin yarısının belirtilen sınıfa ait olduğunu düşünürüz. Buna <em>aratlama</em> (calibration) denir.</p>
<p>Softmaks fonksiyonu 1959 yılında sosyal bilimci R. Duncan Luce tarafından bahsettiğimiz sınıflandırmaya benzer olan <em>seçim modeli</em> için bulundu. Elde edilen $o$ değerlerini negatif olmayan ve toplamları $1$ olan sayılara çevirmek, bunun yanında modelimizin türevlenebilir olmasını şu şekilde sağlayacağız: Bir $o$ sayısının üstel fonksiyon altındaki değerini diğer tüm sayıların üstellerinin toplamına oranlayacağız. Sayının üstelini almak sonucun negatif olmamasını, toplama oranlamak da sonuçların toplamının $1$ olmasını garanti eder. Matemaiksel olarak</p>
$$
\hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)} \quad
 \text{olmak üzere}\quad
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})
$$<p>şeklinde yazabiliriz.</p>
<p>Üçlü örneğimizde her $i$ için $0 \leq \hat{y}_i \leq 1$ ve $\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$ olacağını görebilirsiniz. Dolayısıyla $\hat{y}$ bir olasılık dağılımı belirtir ve $\hat{\mathbf{y}}$  vektörünün değerleri de bu şekilde yorumlanır. Softmaks işlemi büyüklük küçüklük sıralamsını değiştirmez ve yani en olası sınıfı iki yoldan da seçebiliriz:</p>
$$
\hat{\imath}(\mathbf{o}) = \operatorname*{argmax}_i o_i = \operatorname*{argmax}_i \hat y_i.
$$<p>Sonuç olarak $\mathbf{o}$ vektörünü oluşturan sayılar, softmaks öncesi her sınıfın olasılıklarını belirleyen sayılardır. Vektör gösterimi ile ${\mathbf{o}}^{(i)} = \mathbf{W} {\mathbf{x}}^{(i)} + {\mathbf{b}}$,
where ${\hat{\mathbf{y}}}^{(i)} = \mathrm{softmax}({\mathbf{o}}^{(i)})$ şeklinde yazabiliriz.</p>
<h3 id="Vectorization-for-Minibatches">
<a class="anchor" href="#Vectorization-for-Minibatches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vectorization for Minibatches<a class="anchor-link" href="#Vectorization-for-Minibatches"> </a>
</h3>
<p>To improve computational efficiency and take advantage of GPUs,
we typically carry out vector calculations for minibatches of data.
Assume that we are given a minibatch $\mathbf{X}$ of examples
with dimensionality $d$ and batch size $n$.
Moreover, assume that we have $q$ categories (outputs).
Then the minibatch features $\mathbf{X}$ are in $\mathbb{R}^{n \times d}$,
weights $\mathbf{W} \in \mathbb{R}^{d \times q}$,
and the bias satisfies $\mathbf{b} \in \mathbb{R}^q$.</p>
$$
\begin{aligned}
\mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \\
\hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}).
\end{aligned}
$$<p>This accelerates the dominant operation into
a matrix-matrix product $\mathbf{W} \mathbf{X}$
vs the matrix-vector products we would be executing
if we processed one example at a time.
The softmax itself can be computed
by exponentiating all entries in $\mathbf{O}$
and then normalizing them by the sum.</p>
<h2 id="Loss-Function">
<a class="anchor" href="#Loss-Function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss Function<a class="anchor-link" href="#Loss-Function"> </a>
</h2>
<p><img class="emoji" title=":label:" alt=":label:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f7.png" height="20" width="20"><code>section_cross_entropy</code></p>
<p>Next, we need a <em>loss function</em> to measure
the quality of our predicted probabilities.
We will rely on <em>likelihood maximization</em>,
the very same concept that we encountered
when providing a probabilistic justification
for the least squares objective in linear regression
(:numref:<code>sec_linear_regression</code>).</p>
<h3 id="Log-Likelihood">
<a class="anchor" href="#Log-Likelihood" aria-hidden="true"><span class="octicon octicon-link"></span></a>Log-Likelihood<a class="anchor-link" href="#Log-Likelihood"> </a>
</h3>
<p>The softmax function gives us a vector $\hat{\mathbf{y}}$,
which we can interpret as estimated conditional probabilities
of each class given the input $x$, e.g.,
$\hat{y}_1$ = $\hat{P}(y=\mathrm{cat} \mid \mathbf{x})$.
We can compare the estimates with reality
by checking how probable the <em>actual</em> classes are
according to our model, given the features.</p>
$$
P(Y \mid X) = \prod_{i=1}^n P(y^{(i)} \mid x^{(i)})
\text{ and thus }
-\log P(Y \mid X) = \sum_{i=1}^n -\log P(y^{(i)} \mid x^{(i)}).
$$<p>Maximizing $P(Y \mid X)$ (and thus equivalently minimizing $-\log P(Y \mid X)$)
corresponds to predicting the label well.
This yields the loss function
(we dropped the superscript $(i)$ to avoid notation clutter):</p>
$$
l = -\log P(y \mid x) = - \sum_j y_j \log \hat{y}_j.
$$<p>For reasons explained later on, this loss function
is commonly called the <em>cross-entropy</em> loss.
Here, we used that by construction $\hat{y}$
is a discrete probability distribution
and that the vector $\mathbf{y}$ is a one-hot vector.
Hence the sum over all coordinates $j$ vanishes for all but one term.
Since all $\hat{y}_j$ are probabilities,
their logarithm is never larger than $0$.
Consequently, the loss function cannot be minimized any further
if we correctly predict $y$ with <em>certainty</em>,
i.e., if $P(y \mid x) = 1$ for the correct label.
Note that this is often not possible.
For example, there might be label noise in the dataset
(some examples may be mislabeled).
It may also not be possible when the input features
are not sufficiently informative
to classify every example perfectly.</p>
<h3 id="Softmax-and-Derivatives">
<a class="anchor" href="#Softmax-and-Derivatives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmax and Derivatives<a class="anchor-link" href="#Softmax-and-Derivatives"> </a>
</h3>
<p>Since the softmax and the corresponding loss are so common,
it is worth while understanding a bit better how it is computed.
Plugging $o$ into the definition of the loss $l$
and using the definition of the softmax we obtain:</p>
$$
l = -\sum_j y_j \log \hat{y}_j = \sum_j y_j \log \sum_k \exp(o_k) - \sum_j y_j o_j
= \log \sum_k \exp(o_k) - \sum_j y_j o_j.
$$<p>To understand a bit better what is going on,
consider the derivative with respect to $o$. We get</p>
$$
\partial_{o_j} l = \frac{\exp(o_j)}{\sum_k \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j = P(y = j \mid x) - y_j.
$$<p>In other words, the gradient is the difference
between the probability assigned to the true class by our model,
as expressed by the probability $P(y \mid x)$,
and what actually happened, as expressed by $y$.
In this sense, it is very similar to what we saw in regression,
where the gradient was the difference
between the observation $y$ and estimate $\hat{y}$. This is not coincidence.
In any <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential family</a> model,
the gradients of the log-likelihood are given by precisely this term.
This fact makes computing gradients easy in practice.</p>
<h3 id="Cross-Entropy-Loss">
<a class="anchor" href="#Cross-Entropy-Loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-Entropy Loss<a class="anchor-link" href="#Cross-Entropy-Loss"> </a>
</h3>
<p>Now consider the case where we observe not just a single outcome
but an entire distribution over outcomes.
We can use the same representation as before for $y$.
The only difference is that rather than a vector containing only binary entries,
say $(0, 0, 1)$, we now have a generic probability vector, say $(0.1, 0.2, 0.7)$.
The math that we used previously to define the loss $l$ still works out fine,
just that the interpretation is slightly more general.
It is the expected value of the loss for a distribution over labels.</p>
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_j y_j \log \hat{y}_j.
$$<p>This loss is called the cross-entropy loss and it is
one of the most commonly used losses for multiclass classification.
We can demystify the name by introducing the basics of information theory.</p>
<h2 id="Information-Theory-Basics">
<a class="anchor" href="#Information-Theory-Basics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Information Theory Basics<a class="anchor-link" href="#Information-Theory-Basics"> </a>
</h2>
<p>Information theory deals with the problem of encoding, decoding, transmitting
and manipulating information (also known as data) in as concise form as possible.</p>
<h3 id="Entropy">
<a class="anchor" href="#Entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Entropy<a class="anchor-link" href="#Entropy"> </a>
</h3>
<p>The central idea in information theory is to quantify the information content in data.
This quantity places a hard limit on our ability to compress the data.
In information theory, this quantity is called the <a href="https://en.wikipedia.org/wiki/Entropy">entropy</a> of a distribution $p$,
and it is captured by the following equation:</p>
$$
H[p] = \sum_j - p(j) \log p(j).
$$<p>One of the fundamental theorems of information theory states
that in order to encode data drawn randomly from the distribution $p$,
we need at least $H[p]$ "nats" to encode it.
If you wonder what a "nat" is, it is the equivalent of bit
but when using a code with base $e$ rather than one with base 2.
One nat is $\frac{1}{\log(2)} \approx 1.44$ bit.
$H[p] / 2$ is often also called the binary entropy.</p>
<h3 id="Surprisal">
<a class="anchor" href="#Surprisal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Surprisal<a class="anchor-link" href="#Surprisal"> </a>
</h3>
<p>You might be wondering what compression has to do with prediction.
Imagine that we have a stream of data that we want to compress.
If it is always easy for us to predict the next token,
then this data is easy to compress!
Take the extreme example where every token in the stream always takes the same value.
That is a very boring data stream!
And not only it is boring, but it is easy to predict.
Because they are always the same, we do not have to transmit any information
to communicate the contents of the stream.
Easy to predict, easy to compress.</p>
<p>However if we cannot perfectly predict every event,
then we might some times be surprised.
Our surprise is greater when we assigned an event lower probability.
For reasons that we will elaborate in the appendix,
Claude Shannon settled on $\log(1/p(j)) = -\log p(j)$
to quantify one's <em>surprisal</em> at observing an event $j$
having assigned it a (subjective) probability $p(j)$.
The entropy is then the <em>expected surprisal</em>
when one assigned the correct probabilities
(that truly match the data-generating process).
The entropy of the data is then the least surprised
that one can ever be (in expectation).</p>
<h3 id="Cross-Entropy-Revisited">
<a class="anchor" href="#Cross-Entropy-Revisited" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-Entropy Revisited<a class="anchor-link" href="#Cross-Entropy-Revisited"> </a>
</h3>
<p>So if entropy is level of surprise experienced
by someone who knows the true probability,
then you might be wondering, <em>what is cross-entropy?</em>
The cross-entropy <em>from</em> $p$ <em>to</em> $q$, denoted $H(p, q)$,
is the expected surprisal of an observer with subjective probabilities $q$
upon seeing data that was actually generated according to probabilities $p$.
The lowest possible cross-entropy is achieved when $p=q$.
In this case, the cross-entropy from $p$ to $q$ is $H(p, p)= H(p)$.
Relating this back to our classification objective,
even if we get the best possible predictions, we will never be perfect.
Our loss is lower-bounded by the entropy given by the
actual conditional distributions $P(\mathbf{y} \mid \mathbf{x})$.</p>
<h3 id="Kullback-Leibler-Divergence">
<a class="anchor" href="#Kullback-Leibler-Divergence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kullback-Leibler Divergence<a class="anchor-link" href="#Kullback-Leibler-Divergence"> </a>
</h3>
<p>Perhaps the most common way to measure the distance between two distributions
is to calculate the <em>Kullback-Leibler divergence</em> $D(p\|q)$.
This is simply the difference between the cross-entropy and the entropy,
i.e., the additional cross-entropy incurred over the irreducible minimum value it could take:</p>
$$
D(p\|q) = H(p, q) - H[p] = \sum_j p(j) \log \frac{p(j)}{q(j)}.
$$<p>Note that in classification, we do not know the true $p$,
so we cannot compute the entropy directly.
However, because the entropy is out of our control,
minimizing $D(p\|q)$ with respect to $q$
is equivalent to minimizing the cross-entropy loss.</p>
<p>In short, we can think of the cross-entropy classification objective
in two ways: (i) as maximizing the likelihood of the observed data;
and (ii) as minimizing our surprise (and thus the number of bits)
required to communicate the labels.</p>
<h2 id="Model-Prediction-and-Evaluation">
<a class="anchor" href="#Model-Prediction-and-Evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Prediction and Evaluation<a class="anchor-link" href="#Model-Prediction-and-Evaluation"> </a>
</h2>
<p>After training the softmax regression model, given any example features,
we can predict the probability of each output category.
Normally, we use the category with the highest predicted probability as the output category. The prediction is correct if it is consistent with the actual category (label).
In the next part of the experiment,
we will use accuracy to evaluate the model’s performance.
This is equal to the ratio between the number of correct predictions and the total number of predictions.</p>
<h2 id="Summary">
<a class="anchor" href="#Summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary<a class="anchor-link" href="#Summary"> </a>
</h2>
<ul>
<li>We introduced the softmax operation which takes a vector and maps it into probabilities.</li>
<li>Softmax regression applies to classification problems. It uses the probability distribution of the output category in the softmax operation.</li>
<li>Cross-entropy is a good measure of the difference between two probability distributions. It measures the number of bits needed to encode the data given our model.</li>
</ul>
<h2 id="Exercises">
<a class="anchor" href="#Exercises" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exercises<a class="anchor-link" href="#Exercises"> </a>
</h2>
<ol>
<li>Show that the Kullback-Leibler divergence $D(p\|q)$ is nonnegative for all distributions $p$ and $q$. Hint: use Jensen's inequality, i.e., use the fact that $-\log x$ is a convex function.</li>
<li>Show that $\log \sum_j \exp(o_j)$ is a convex function in $o$.</li>
<li>We can explore the connection between exponential families and the softmax in some more depth<ul>
<li>Compute the second derivative of the cross-entropy loss $l(y,\hat{y})$ for the softmax.</li>
<li>Compute the variance of the distribution given by $\mathrm{softmax}(o)$ and show that it matches the second derivative computed above.</li>
</ul>
</li>
<li>Assume that we have three classes which occur with equal probability, i.e., the probability vector is $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$.<ul>
<li>What is the problem if we try to design a binary code for it? Can we match the entropy lower bound on the number of bits?</li>
<li>Can you design a better code. Hint: what happens if we try to encode two independent observations? What if we encode $n$ observations jointly?</li>
</ul>
</li>
<li>Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as $\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$.<ul>
<li>Prove that $\mathrm{RealSoftMax}(a, b) &gt; \mathrm{max}(a, b)$.</li>
<li>Prove that this holds for $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$, provided that $\lambda &gt; 0$.</li>
<li>Show that for $\lambda \to \infty$ we have $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$.</li>
<li>What does the soft-min look like?</li>
<li>Extend this to more than two numbers.</li>
</ul>
</li>
</ol>
<p><a href="https://discuss.d2l.ai/t/46">Discussions</a></p>

</div>
</div>
</div>
</div>



  </div>
<!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js" repo="unverciftci/derin_ogrenme" issue-term="title" label="blogpost-comment" theme="github-light" crossorigin="anonymous" async>
</script><a class="u-url" href="/derin_ogrenme/jupyter/2020/02/21/Linear-Siniflandirma.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/derin_ogrenme/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/derin_ogrenme/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/derin_ogrenme/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Derin Öğrenmeye Giriş</p>
      </div>
    </div>

    <div class="social-links">
<ul class="social-media-list">
<li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/derin_ogrenme/assets/minima-social-icons.svg#github"></use></svg></a></li>
<li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/derin_ogrenme/assets/minima-social-icons.svg#twitter"></use></svg></a></li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
