{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "2020-02-25-Linear-Regresyon.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEi8vllHz-21",
        "colab_type": "text"
      },
      "source": [
        "# Lineer Regresyon (✗)\n",
        "> En basit regresyon modeli.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [jupyter]\n",
        "- image: images/chart-preview.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "Nw_TTGVvzyfp",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "*Regresyon*, bir veya daha fazla bağımsız değişkenle bir bağımlı değişkenin ilişkisini modelleyen metodlara verilen genel isimdir. Doğa bilimlerinde ve sosyal bilimlerde regresyonun amacı genellikle girdiler ve çıktılar arasındaki bağı karakterize etmektir. Buna karşın Makine Öğrenmesi ise çoğunlukla *tahmin* ile ilgilidir. \n",
        "\n",
        "Ne zaman sayısal bir değeri tahmin etmek istesek regresyon karşımıza çıkar. Yaygın örnekler arasında fiyat tahmini (ev, hisse senedi gibi), bir yerde kalma süresinin tahmini (hastanede kalma süresi gibi), arz tahmini (emlak satışları için) vardır. Her tahmin problemi regresyon problemi değildir. İleriki bölümlerde sınıflandırma problemlerini inceleyeceğiz. Sınıflandırmada amaç kategoriler arasından doğru olanı tahmin etmedir, dolayısıyla regresyon problemi değildir.\n",
        "\n",
        "## Lineer Regresyonun Temel Ögeleri\n",
        "\n",
        "*Lineer regresyon*, standart regresyon araçları arasında hem en popüler hem de en basit olanıdır. Geçmişi 19. Yüzyıla dayanan lineer regresyon basit bir kabülden yola çıkar. Öncelikle $\\mathbf{x}$ bağımsız değişkenleri ile $y$ bağımlı değişkeninin lineer olarak bağlantılı olduğunu kabul ederiz. Yani $y$ değişkeni $\\mathbf{x}$ vektörünün bileşenlerinin lineer birleşimi ile gözlem hatasınının toplamıdır. İkincisi gözlem hatası düzenlidir, özel olarak Gauss dağılımına sahiptir.          \n",
        "\n",
        "Amacımızı daha iyi ifade etmek için bir örnek kullanalım. Ev fiyatlarını alanına ve yaşına göre tahmin etmek istiyoruz. Bu kestirimi yapabilmek amacıyla uygulayacağımız model için fiyatını, alanını ve yaşını bildiğimiz satış verisine (data set) ihtiyacımız var. Makine Öğrenmesi terminolojisinde veriye *eğitim seti* (training set) ve bu kümenin elemanlarına (bu örnekte her bir satış oluyor) bir *örnek* ya da *veri noktası* denir. Bu örnekteki fiyat gibi tahmin etmeye çalıştığımız şeye *etiket* (label) ya da *hedef* (target) denir. Örneğimizdeki evin alanı ve yaşı gibi bağımsız değişkenlere *özellikler* (features) denir.  \n",
        "\n",
        "Veri kümemizdeki örneklerin sayısını $n$ ile gösteriyoruz. Veri örneklerini indekslemek için $i$ harfini kullanacağız. Dolayısyla $y^{(i)}$ etiketine karşılık gelen özellikler $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$ ile gösterilecektir. \n",
        "\n",
        "### Lineer Model\n",
        "\n",
        "Lineerlik kabulü gereği hedef (fiyat), özelliklerin (alan ve yıl) lineer birleşimidir:\n",
        "\n",
        "$$fiyat = w_{\\textrm{alan}} \\cdot \\textrm{alan} + w_\\textrm{yıl} \\cdot \\textrm{yıl} + b.$$\n",
        "\n",
        "Burada , $w_{\\textrm{alan}}$ and $w_{\\textrm{yıl}}$ katsayılarına *ağırlıklar*, $b$ sayısına called a *yanlılık* denir. Ağırlıklar herbir özelliğin (feature) tahminimiz üzerinde ne kadar etkili olduğunu, yanlılık terimi de teüm üzellikler sıfır olduğunda tahmin edilen fiyatın kaç olduğunu belirler. Evin alanı veya yaşı sıfır olmaz ama yanılık terimi modelimizin temsil yeterliliğinin kısıtlanmaması açısından önemlidir.  \n",
        "\n",
        "Amacımız, elimizdeki veri için $\\mathbf{w}$ ağırlılarını $b$ yanlılığını uygun şekilde belirlemektir. Bu sayede modelimizin yaptığı tahminler ile veri kümesindeki gerçek değerler ortalama olarak örtüşmelidir.\n",
        "\n",
        "Makine öğrenmesinde yüksek boyutlu veri kümeleri ile çalığılır, yani özelliklerin sayısı çok fazladır. Örneğin bir fotoğrafın her bir pikseli bir özellik olarak alınır. Dolayısıyla matrisler ve vektörler gibi Lineer Cebir araçlarını kullanmak yararlı olur. Herbir veri (girdi), $d$ özelliğe sahip ise $\\hat{y}$ tahminimizi  \n",
        "\n",
        "$$\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$\n",
        "\n",
        "şeklinde ifade ederiz. Tüm özellikleri $\\mathbf{x} \\in \\mathbb{R}^d$ vektörünün bileşenleri ile, tüm ağırlıkları da $\\mathbf{w} \\in \\mathbb{R}^d$ vektörünün bileşnleri ile gösterirsek, modelimizi \n",
        "\n",
        "$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$$\n",
        "\n",
        "nokta çarpımıyla ifade etmiş oluruz. \n",
        "\n",
        "Burada $\\mathbf{x}$ vektörü bir tek veri örneğine karşılık gelen özellik vektörüdür. Veri kümesi $n$ örnekten oluşsun ve hepsini $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ matrisinde bir araya getirelim. Burada herbir $\\mathbf{x}$ vektörü $\\mathbf{X}$ matrisinin bir sütununu oluşturmaktadır. $\\mathbf{X}$ matrisine *dizayn matrisi* veya *veri matrisi* denir.   \n",
        "\n",
        "Özelliklerin bir araya getirilmesi ile oluşan $\\mathbf{X}$ için  $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ tahminimiz matris-vektör çarımıyla\n",
        "\n",
        "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b,$$\n",
        "\n",
        "biçiminde ifade edilebilir. Burada $b$ vektörü aslında aynı $b$ sayısının $n$ defa yazılmasıyla oluşan vektördür. Python programlamadaki yayımlama (broadcasting) gibi. Lineer regresyonda amaç, $\\mathbf{X}$ eğitim kümemizdeki özellikler ve karşılık gelen $\\mathbf{y}$ etiketleri için, aynı dağılımdan gelen yeni bir örnek için tahmindeki hatayı beklenen değer olarak en az yapan $\\mathbf{w}$  ve $b$ terimlerini bulmaktır.    \n",
        "\n",
        "$\\mathbf{x}$ verildiğinde $y$ için en iyi tahmini veren modelin lineer olduğunu düşünsek de, her $1 \\leq i \\leq n$ için gerçek $y^{(i)}$ ile $\\mathbf{w}^\\top \\mathbf{x}^{(i)}+b$ tahmininin eşit olmasını bekleyemeyiz. Örneğin $\\mathbf{X}$ özellikleri ve $\\mathbf{y}$ etiketleri küçük ölçüm hataları yaılmış olabilir. Dolayısıyla, geri landa yatan ilişkinin lineer olduğundan emin olsak bile bu tür küçük hataları ifade eden bir gürültü (noise) terimini de dahil edeceğiz. \n",
        "\n",
        "En iyi $\\mathbf{w}$ and $b$ parametrelerini aramaya başlamadan önce iki şey yapmalıyız: (i) modelin kalitesini ölçen bir araç ve (ii) modelin kalitesini iyileştirmek için izlenecek yol.  \n",
        "\n",
        "### Kayıp Fonksiyonu (Loss Function)\n",
        "\n",
        "Modelimizi veriye uydurmaya başlamadan önce *uyum* (fitness) için bir ölçeğe ihtiyacımız var. *Kayıp fonksiyonu* sayesinde *gerçek* ve *tahmini* değer arasındaki mesafeyi ölçeriz. Kayıp genellikle negatif olmayan bir sayıdır ve küçük olması istenir hatta mükemmel sonuç için sıfırdır. Regresyon problemlerinde en çok kullanılan kayıp fonksiyonu hataların karelerinin toplamıdır. Buna *en küçük kareler hata fonksiyonu* da denir. Eğer $i$ numaralı örneğimiz için tahminimiz $\\hat{y}^{(i)}$ ve karşılık gelen doğru etiket $y^{(i)}$ ise, hatanın karesi \n",
        "\n",
        "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
        "\n",
        "olur. \n",
        "\n",
        "Burada $1/2$ katsayısı sonucu değiştirmez ama türev alındığında sadeleşeceği için işlem kolaylığı sağlar. Eğitim kümemiz hazır olarak verildiğinden bizim kontrolümüz dışındadır, dolayısıyla yukarıdaki deneysel hata (emprical losss) sadece model parametrelerinin fonksiyonudur. Anlatılanları somutlaştırmak adına bir boyutlu durum için regresyon problemini aşağıdaki grafikte gösterelim.   \n",
        "\n",
        "![Fit data with a linear model.](https://d2l.ai/_images/fit_linreg.svg)\n",
        "\n",
        "Burada hataların kareleri alındığından büyük hatların kayıp fonksiyonunu çok fazla büyüteceğine dikkat edin. Tüm veri kümesi üzerinden modelin kalitesini ölçmek için hataların ortalamasını (ya da sadece toplamını) alırız:\n",
        "\n",
        "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
        "\n",
        "Modeli eğitirken, parametrelerin ($\\mathbf{w}^*, b^*$) toplam kayıbı en küçük yapmasını yani\n",
        "\n",
        "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b)$$\n",
        "\n",
        "istiyoruz.\n",
        "\n",
        "### Analitik Çözüm\n",
        "\n",
        "Lineer Regreyon az rastlanır ölçüde basit bir optimizasyon problemdir. Karşılaşacağımız birçok modelin aksine lineer regresyon analitik bir şekilde çözülür ve global optimum bulunur. İlk olarak $b$ yanlılık terimini $\\mathbf{w}$ arametrelerine eklyelelim buna karşılık dizayn matrisine $1$ sayılarından oluşan bir sütun ekleyelim. Bu durumda tahmin problemimiz $||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||^2$ ifadesini minimize etmeye dönüşür. Bu ikinci dereceden bir ifade olduğu için konvekstir.  \n",
        "\n",
        "Dolayısıyla kayıp fonksiyonunun belirttiği yüzey üzerinde bir tek kritik nokta vardır, bu da global minimuma karşılık gelir. Kayıp fonksiyonunun $\\mathbf{w}$ üzerinden türevini alıp sıfıra eşitlersek,  \n",
        "\n",
        "$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}$$\n",
        "\n",
        "analitik çözümünü elde ederiz. Analitik çözüm matematiksel analiz için yararlı olsa da matrisin tersin içerdiğinden pratikte kullanışsızdır. Derin öğrenmede bu şekilde analitik çözümler neredeyse hiç kullanılmaz.\n",
        "\n",
        "### Gradient Azalım (Gradient descent)\n",
        "\n",
        "Modelimizi analitik olarak çözemezek veya kayıp yüzeyimiz yüksek boyutlu ve konveks olmasa bile pratikte modelimizi etkin bir biçimde eğitebiliriz. Hatta birçok iş için bu optimize edilmesi güç görünen modeller daha kolay eğitilebilirler ve daha kullanılışlı olurlar.    \n",
        "\n",
        "Neredeyse tüm Derin Öğrenme modellerinde optimizasyon için kullanılan kilit yöntem, hatayı azaltmak için parametrelerin kayıp fonksiyonunun azaldığı yönde güncellendiği itreratif yöntemdir. Bu algoritmaya *gradiyent azalım* denir. Konveks yüzeyler için global minimuma eninde sonunda ulaşılır fakat konveks olmayanlar için (genellikle yeterince iyi olan) bir yerel minimuma yönelir.  \n",
        "\n",
        "Gradiyent azalımı kullanırken kayıp fonksiyonunun verinin tamamı üzerinden türevini almak kaba bir yaklaşım olur çünkü bu veri kümesindeki her bir örnek için elde edilen kayıp değerlerinin ortlamasıdır. Her güncelleme adımında veri kümesinin tamamını tarayacağımızdan hesap çok uzun sürer. Genellikle her güncellemede verinin tamamı yerine örneklerden rastgele bir *yığın* (batch) alınır. Buna *stokastik gradiyent azalım* denir.      \n",
        "\n",
        "Her ötelemede (iteration) belli sayıda örnekten oluşan rastgele küçük bir $\\mathcal{B}$ yığını seçilir. Daha sonra yığının ortalama kayıbının model parametrelerine göre türevi (gradiyenti) hesaplanır. Son olarak da gradiyent belli bir $\\eta > 0$ adım boyu ile çarpılıp güncel parametre değerinden çıkarılır.    \n",
        "\n",
        "Güncellemeyi matematiksel olarak şöyle ifade edebiliriz: \n",
        "\n",
        "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n",
        "\n",
        "Burada $\\partial$ ile kısmi türev gösterilmektedir. Algoritmanın adımları: (i) parametreler için başlangıç değerleri seçmek, genellikle rastgele seçilir (ii) veriden her bir adım için rastgele yığınlar seçmek ve parametrleri negatif gradiyent doğrultusunda güncellemek olarak özetlenebilir.  \n",
        "\n",
        "İkinci dereceden kayıplar ve lineer fonksiyonlar için güncellemeyi açık olarak bulabiliriz. Burada $\\mathbf{w}$ ve $\\mathbf{x}$ vektörlerdir. $w_1, w_2, \\ldots, w_d$ bileşenlerini kullanmak yerine vektör gösterimi matematiksel ifadeleri daha okunaklı yapar. Sonuç olarak güncelleme\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && =\n",
        "\\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\\n",
        "b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  && =\n",
        "b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "biçiminde ifade edilebilir. Yukarıdaki eşitliklerde $|\\mathcal{B}|$ ile *yığın boyutu* (batch size) ve $\\eta$ ile de *öğrenme parametresi* (learining rate) gösterilmektedir. Yığın boyutu ve öğrenme parametresini önceden belirlediğimizi, bunların öğrenilmediğini vurgulamak gerekir. Bu şekilde değiştirilebilen fakat öğrenme sırasında güncellenmeyen parametrelere *hiperparametre* denir. *Hiperparametre ayarlama* (hyerparameter tuning), bunların uygun biçimde seçilmesi işlemidir ve eğitim kümesi ile elde edilen sonuçların ayrı bir *doğrulama kümesi* (validation set) ile denenmesi sayesinde yapılabilir.\n",
        "\n",
        "Belli sayıda güncellemeden sonra ya da belli şartlar sağlandığında eğitme (training) durdurulur. Sonra bulduğumuz parametreler kaydedilir. Bu parametreler gerçek parametrelerin tamin edilen değerleridir ve $\\hat{\\mathbf{w}}, \\hat{b}$ ile gösterilirler. Fonksiyonumuz tam olarak lineer olsa ve hiçbir ölçüm hatası olmasa bile elde ettiğimiz bu parametreler tam olarak doğru değildir çünkü algoritmamız sonlu sayıda adımda minimuma ulaşamaz. \n",
        "\n",
        "Lineer regresyon, konveks öğrenme problemi olduğundan bir tek (global) minimumu vardır. Daha kamaşık derin ağlarda kayıp yüzeyi üzerinde birçok minimum vardır. Neyse ki derin öğrenmede uygulayıcılar eğitim kümesi üzerinde minimumu bulmada fazla zorlanmazlar ve bunun matematiksel temeli halen çok iyi bilinmemektedir. Daha zor olanı, daha önce kullanmadığımız örnekler üzerinde düşük kayıp değeri veren parametreler bulmaktır. Buna probleme *genelleştirme* denir. İleride bu konulara değineceğiz.  \n",
        "\n",
        "### Öğrenilen Model ile Tahminler Yapma (Estimation)\n",
        "\n",
        "Öğrenimliş bir $\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$ lineer regresyon modeli ile eğriyim kümemizde yer almayan örnekler için tahminde bulunabiliriz. Örneğin alanı $x_1$ ve yılı $x_2$ olan bir evin fiyatını tahmin edebiliriz. Özellikleri verilen hedefi tahmin etme işine *kestirim* (prediction) veya *çıkarım* (inference) denir. \n",
        "\n",
        "İstatistikte genellikle çıkarım ile parametrelerin veri kullanılarak tahmini ifade edilir. Dolayısıyla karışıklığa yol açmaması için derin öğrenme için kestirim kullanılacaktır.  \n",
        "\n",
        "### Vektörleştirme Hızlandırır\n",
        "\n",
        "Modeli eğitirken tüm küçük veri yığınlarını aynı anda işlemek isteriz. Bunu etkin bir şekilde yamanın yolu, işlemleri vektörleştirmek ve hızlı döngüler (*for loop* gibi) hızlı lineer cebir kütüphanelerini kullanmaktır.  \n",
        "\n",
        "Vektörleştirmeye ihtiyacın daha iyi anlaşılması için vektörlerin toplamı için iki metod kullanalım. Önce $1$ rakamlarından oluşan $10000$ boyutlu vektörler alalım. İlk metod için Python'da vektörün bileşenleri üzerinden `for` döngüsü oluşturacağız. İkincisinde ise sadece bir kez `+` işlemcisini kullanacağız.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBTKdKcIfSsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U mxnet-cu101mkl==1.6.0  # updating mxnet to at least v1.6\n",
        "!pip install d2l==0.13.2 -f https://d2l.ai/whl.html # installing d2l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 1,
        "tab": "mxnet",
        "id": "CdtTv2V8zyfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from d2l import mxnet as d2l\n",
        "import math\n",
        "from mxnet import np\n",
        "import time\n",
        "\n",
        "n = 10000\n",
        "a = np.ones(n)\n",
        "b = np.ones(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 3,
        "id": "Xd4rYk1Yzyf4",
        "colab_type": "text"
      },
      "source": [
        "Zaman açısından kıyasamayı ileride de sık sık yapacağımızdan, bunun için `d2l` paketinde erişilmek üzere bir kronometre (timer) sınıfı tanımlayalım. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 4,
        "tab": "all",
        "id": "44uuM34Yzyf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Timer:  #@save\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated times.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "RZrDlZhLzygF",
        "colab_type": "text"
      },
      "source": [
        "Şimdi işleri karşılaştırabiliriz. Önce `for` döngüsüyle bileşen bileşen toplama yapalım. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 6,
        "tab": "mxnet",
        "id": "Jx7k0YQhzygH",
        "colab_type": "code",
        "outputId": "413f6678-973f-4e0b-e6bc-6034cbdae86d",
        "colab": {}
      },
      "source": [
        "c = np.zeros(n)\n",
        "timer = Timer()\n",
        "for i in range(n):\n",
        "    c[i] = a[i] + b[i]\n",
        "f'{timer.stop():.5f} sec'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.18153 sec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "OJ7V2INqzygT",
        "colab_type": "text"
      },
      "source": [
        "Şimdi de `mxnet.np` (yukarıda çağırdık) yeniden tanımlanan `+` işlemcisini kullanalım. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 9,
        "tab": "mxnet",
        "id": "mXQiP8TgzygV",
        "colab_type": "code",
        "outputId": "3cabd529-1078-4ab5-f42e-cb1d3dd15b55",
        "colab": {}
      },
      "source": [
        "timer.start()\n",
        "d = a + b\n",
        "f'{timer.stop():.5f} sec'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.00025 sec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "lcJua3f3zygh",
        "colab_type": "text"
      },
      "source": [
        "İkincisinin farklı şekilde hızlı olduğu görülüyor. Kodu vektörleştirmek genellikle hızda üstel olarak değişen azalmalar sağlar. Hatta, işlemleri hazır kütüphaneleri kullanarak yaptırdığımızdan hataların da azalmasını sağlamış oluruz.  \n",
        "\n",
        "## Normal Dağılım ve En Küçük Kareler Kayıp Fonksiyonu\n",
        "\n",
        "Şimdiden lineer regresyonu uygulayacak kadar bilgi edinmiş olsak bile, en küçük kareler kayıp fonksiyonunun neden gürültü ile ilgili kabulümüz ile uyumlu olduğunu bilmek yayarlı olacaktır.\n",
        "\n",
        "Yukarıda $l(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$ olarak tanılmalan en küçük kareler kayıp fonksiyonunun yararlı özellikleri görüldü. Bunlardan birisi de türevinin $\\partial_{\\hat{y}} l(y, \\hat{y}) = (\\hat{y} - y)$ şeklinde çok sade olmasıdır. \n",
        "\n",
        "Lineer regresyonu Gauss 1975 yılında buldu. Gauss aynı zamanda normal dağılımı (Gauss dağılımı da denir) da keşfeden kişidir. Yani normal dağılımla lineer regresyon arasında babalarının aynı olması ötesinde de derin bağlar vardır. Şimdi ortalamsı (mean) $\\mu$ and varyansı $\\sigma^2$ olan normal dağılımın yoğunluk fonksiyonunun \n",
        "\n",
        "$$p(z) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (z - \\mu)^2\\right)$$\n",
        "\n",
        "olduğunu hatırlayalım. Aşağıda normal dağılımı hesalayan Python fonksiyonu veriliyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 12,
        "tab": "all",
        "id": "FtCOIL-szygj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal(z, mu, sigma):\n",
        "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
        "    return p * np.exp(- 0.5 / sigma**2 * (z - mu)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 13,
        "id": "k7URWMsIzygs",
        "colab_type": "text"
      },
      "source": [
        "Bununla normal dağılımı görselleştirebiliriz. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 14,
        "tab": "all",
        "id": "6e60dW55zygt",
        "colab_type": "code",
        "outputId": "2475bf33-b2e8-4126-df6b-a0abcdf83163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "# Using numpy again for visualizations.\n",
        "x = np.arange(-7, 7, 0.01)\n",
        "\n",
        "# Mean and variance pairs\n",
        "parameters = [(0, 1), (0, 2), (3, 1)]\n",
        "d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in parameters], xlabel='z',\n",
        "         ylabel='p(z)', figsize=(4.5, 2.5),\n",
        "         legend=['ort %d, var %d' % (mu, sigma) for mu, sigma in parameters]);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 324x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 302.08125 180.65625\" width=\"302.08125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 302.08125 180.65625 \nL 302.08125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 143.1 \nL 294.88125 143.1 \nL 294.88125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 71.511736 143.1 \nL 71.511736 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m014f73f44f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"71.511736\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −6 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(64.140643 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 104.145436 143.1 \nL 104.145436 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.145436\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(96.774343 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 136.779136 143.1 \nL 136.779136 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.779136\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(129.408042 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 169.412836 143.1 \nL 169.412836 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"169.412836\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(166.231586 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 202.046536 143.1 \nL 202.046536 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.046536\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2 -->\n      <g transform=\"translate(198.865286 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 234.680236 143.1 \nL 234.680236 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.680236\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 4 -->\n      <g transform=\"translate(231.498986 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 267.313936 143.1 \nL 267.313936 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"267.313936\" xlink:href=\"#m014f73f44f\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6 -->\n      <g transform=\"translate(264.132686 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- z -->\n     <defs>\n      <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n     </defs>\n     <g transform=\"translate(166.707031 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-122\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 43.78125 136.922727 \nL 294.88125 136.922727 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mc493c6866f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc493c6866f\" y=\"136.922727\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 140.721946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 43.78125 105.954475 \nL 294.88125 105.954475 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc493c6866f\" y=\"105.954475\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(20.878125 109.753694)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 43.78125 74.986223 \nL 294.88125 74.986223 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc493c6866f\" y=\"74.986223\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 78.785442)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 43.78125 44.017971 \nL 294.88125 44.017971 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc493c6866f\" y=\"44.017971\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 47.81719)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p577ab8d339)\" d=\"M 43.78125 13.049719 \nL 294.88125 13.049719 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc493c6866f\" y=\"13.049719\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 16.848938)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- p(z) -->\n     <defs>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n      <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n     </defs>\n     <g transform=\"translate(14.798438 84.85)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"102.490234\" xlink:href=\"#DejaVuSans-122\"/>\n      <use x=\"154.980469\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#p577ab8d339)\" d=\"M 55.194886 136.922727 \nL 108.224649 136.813535 \nL 113.609208 136.566288 \nL 117.198915 136.184417 \nL 119.972781 135.668952 \nL 122.257142 135.025185 \nL 124.215162 134.257847 \nL 126.01001 133.330371 \nL 127.804866 132.138337 \nL 129.436549 130.77945 \nL 131.06824 129.113083 \nL 132.699924 127.093511 \nL 134.331607 124.674772 \nL 135.963291 121.812697 \nL 137.594974 118.467299 \nL 139.226665 114.605492 \nL 141.021513 109.733828 \nL 142.816369 104.197086 \nL 144.774389 97.412526 \nL 146.895581 89.247641 \nL 149.506275 78.224543 \nL 153.259151 61.239325 \nL 157.501535 42.275262 \nL 159.785892 33.113073 \nL 161.580748 26.820509 \nL 163.049259 22.424539 \nL 164.354614 19.173265 \nL 165.496788 16.88464 \nL 166.475798 15.362588 \nL 167.454808 14.263614 \nL 168.270654 13.679586 \nL 169.086499 13.401979 \nL 169.739173 13.401979 \nL 170.391846 13.599458 \nL 171.207684 14.122462 \nL 172.02353 14.948572 \nL 173.00254 16.33118 \nL 173.98155 18.126548 \nL 175.123732 20.717339 \nL 176.429079 24.286967 \nL 177.897598 29.000681 \nL 179.692446 35.615337 \nL 181.813638 44.367147 \nL 184.91384 58.245054 \nL 190.461572 83.161144 \nL 192.909093 93.115019 \nL 195.030286 100.899654 \nL 196.988306 107.299468 \nL 198.783169 112.473252 \nL 200.578017 116.986085 \nL 202.209708 120.534576 \nL 203.841384 123.58549 \nL 205.473075 126.176452 \nL 207.104751 128.35022 \nL 208.736442 130.152332 \nL 210.368133 131.62881 \nL 211.999809 132.824477 \nL 213.794657 133.865798 \nL 215.752693 134.73293 \nL 217.873869 135.421683 \nL 220.321406 135.972059 \nL 223.258444 136.389278 \nL 227.011312 136.67952 \nL 232.395871 136.850878 \nL 243.001818 136.917995 \nL 283.467614 136.922727 \nL 283.467614 136.922727 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path clip-path=\"url(#p577ab8d339)\" d=\"M 55.194886 136.7876 \nL 65.80084 136.522951 \nL 73.14342 136.126434 \nL 79.017488 135.590287 \nL 83.912546 134.926494 \nL 88.318087 134.10523 \nL 92.234135 133.153577 \nL 95.823839 132.063161 \nL 99.250378 130.79875 \nL 102.513745 129.367712 \nL 105.77712 127.695009 \nL 109.04049 125.764112 \nL 112.303861 123.563438 \nL 115.567228 121.0879 \nL 118.993767 118.196043 \nL 122.583479 114.860884 \nL 126.499519 110.903145 \nL 131.06824 105.948686 \nL 138.247647 97.770815 \nL 144.611224 90.644946 \nL 148.527265 86.58968 \nL 151.790639 83.53067 \nL 154.564497 81.224439 \nL 157.175199 79.344213 \nL 159.459555 77.957411 \nL 161.743912 76.832369 \nL 163.865105 76.036198 \nL 165.823125 75.522599 \nL 167.781145 75.22717 \nL 169.739173 75.153087 \nL 171.697193 75.301157 \nL 173.655213 75.669778 \nL 175.613233 76.254996 \nL 177.734426 77.126086 \nL 179.855618 78.233158 \nL 182.139975 79.673624 \nL 184.587504 81.480058 \nL 187.361362 83.820856 \nL 190.461572 86.751108 \nL 194.051283 90.469341 \nL 198.783169 95.721754 \nL 211.67348 110.215095 \nL 215.752693 114.383395 \nL 219.505561 117.905417 \nL 222.9321 120.825256 \nL 226.195467 123.328266 \nL 229.458833 125.556318 \nL 232.722216 127.513772 \nL 235.985583 129.211617 \nL 239.24895 130.665968 \nL 242.675489 131.952578 \nL 246.2652 133.063568 \nL 250.18124 134.03448 \nL 254.423625 134.846714 \nL 259.155511 135.514669 \nL 264.54007 136.040363 \nL 270.903632 136.432353 \nL 279.225229 136.707949 \nL 283.467614 136.785216 \nL 283.467614 136.785216 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#p577ab8d339)\" d=\"M 55.194886 136.922727 \nL 157.175199 136.813535 \nL 162.559758 136.566288 \nL 166.149461 136.184417 \nL 168.923327 135.668953 \nL 171.207684 135.025188 \nL 173.165712 134.257847 \nL 174.96056 133.330371 \nL 176.755415 132.138337 \nL 178.387099 130.77945 \nL 180.018782 129.113093 \nL 181.650474 127.093511 \nL 183.282157 124.674772 \nL 184.91384 121.812697 \nL 186.545532 118.467281 \nL 188.177207 114.605511 \nL 189.972071 109.733805 \nL 191.766919 104.197086 \nL 193.724939 97.412526 \nL 195.846131 89.247641 \nL 198.456825 78.224543 \nL 202.209708 61.239288 \nL 206.452077 42.27529 \nL 208.736442 33.113073 \nL 210.53129 26.820546 \nL 211.999809 22.424539 \nL 213.305156 19.173284 \nL 214.447346 16.884622 \nL 215.426348 15.362588 \nL 216.405366 14.263605 \nL 217.221196 13.679596 \nL 218.037042 13.401979 \nL 218.689715 13.401979 \nL 219.342388 13.599449 \nL 220.158234 14.122462 \nL 220.97408 14.948572 \nL 221.953082 16.331171 \nL 222.9321 18.126548 \nL 224.074274 20.71733 \nL 225.379621 24.286948 \nL 226.84814 29.000654 \nL 228.643003 35.615364 \nL 230.764196 44.367184 \nL 233.86439 58.245054 \nL 239.412122 83.161144 \nL 241.859643 93.115019 \nL 243.980835 100.899654 \nL 245.938856 107.299468 \nL 247.733719 112.473252 \nL 249.528567 116.986085 \nL 251.160243 120.534544 \nL 252.791934 123.58549 \nL 254.423625 126.176452 \nL 256.055301 128.35022 \nL 257.686992 130.152332 \nL 259.318668 131.628798 \nL 260.950359 132.824477 \nL 262.745207 133.865798 \nL 264.703242 134.73293 \nL 266.824419 135.421683 \nL 269.271956 135.972059 \nL 272.208978 136.389276 \nL 275.961862 136.67952 \nL 281.346421 136.850878 \nL 283.467614 136.879593 \nL 283.467614 136.879593 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 143.1 \nL 43.78125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 294.88125 143.1 \nL 294.88125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 143.1 \nL 294.88125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 294.88125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 59.234375 \nL 138.526563 59.234375 \nQ 140.526563 59.234375 140.526563 57.234375 \nL 140.526563 14.2 \nQ 140.526563 12.2 138.526563 12.2 \nL 50.78125 12.2 \nQ 48.78125 12.2 48.78125 14.2 \nL 48.78125 57.234375 \nQ 48.78125 59.234375 50.78125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_28\">\n     <path d=\"M 52.78125 20.298438 \nL 72.78125 20.298438 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_29\"/>\n    <g id=\"text_15\">\n     <!-- ort 0, var 1 -->\n     <defs>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     </defs>\n     <g transform=\"translate(80.78125 23.798438)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"61.181641\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.294922\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"141.503906\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"173.291016\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"236.914062\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"268.701172\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"300.488281\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"359.667969\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"420.947266\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"462.060547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"493.847656\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n    <g id=\"line2d_30\">\n     <path d=\"M 52.78125 34.976562 \nL 72.78125 34.976562 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_31\"/>\n    <g id=\"text_16\">\n     <!-- ort 0, var 2 -->\n     <g transform=\"translate(80.78125 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"61.181641\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.294922\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"141.503906\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"173.291016\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"236.914062\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"268.701172\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"300.488281\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"359.667969\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"420.947266\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"462.060547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"493.847656\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 52.78125 49.654688 \nL 72.78125 49.654688 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\"/>\n    <g id=\"text_17\">\n     <!-- ort 3, var 1 -->\n     <g transform=\"translate(80.78125 53.154688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"61.181641\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.294922\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"141.503906\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"173.291016\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"236.914062\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"268.701172\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"300.488281\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"359.667969\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"420.947266\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"462.060547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"493.847656\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p577ab8d339\">\n   <rect height=\"135.9\" width=\"251.1\" x=\"43.78125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 15,
        "id": "S1vORuJhzyg5",
        "colab_type": "text"
      },
      "source": [
        "Görüldüğü gibi ortalamayı değiştirmek $x$ ekseninde kaymalara, yaryansı değiştirmek dağılımın genişlemesine veya daralmasına sebep oluyor. \n",
        "\n",
        "Lineer regresyonda en küçük karelerin kullanımını açıklamak için, gözlemlerdeki hataların gürültülü olduğunu ve bu gürültünün normal dağılımdan geldiğini kabul edelim:  \n",
        "\n",
        "$$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\text{ olmak üzere } y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon.$$\n",
        "\n",
        "Dolayısıyla verilen bir $\\mathbf{x}$ için özel bir $y$ görmenin olabililirlik (likelihood) fonksiyonunu \n",
        "\n",
        "$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right)$$\n",
        "\n",
        "şeklinde yazabiliriz. Buradan maksimum olabilirlik prensibine göre $b$ ve $\\mathbf{w}$ parametrelerinin en iyi değerleri veri kümesinin olabilirliğini en büyük yapanlardır:\n",
        "\n",
        "$$P(Y\\mid X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).$$\n",
        "\n",
        "Maksimum olabilirlik prensibine göre seçilen kestiricilere (estimators) *maksimum olabilirlik kestiricileri* (MLE) denir. Üstel fonksiyonların çarpımını maksimum yapmak yerine genellikle logaritması maksimum yaılır. Optimizasyonda alışkanlık olarak maksimum yerine minimum incelendiğinden olabilirlik fonksiyonunun  negatifinin minimumunu arayacağız (NLL - Negative Log-Likelihood): $-\\log p(\\mathbf y|\\mathbf X)$. İşlem yapılırsa    \n",
        "\n",
        "$$-\\log p(\\mathbf y|\\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2$$\n",
        "\n",
        "elde edilir. Şimdi yapacağımız son kabul $\\sigma$ yani varyansın sabit olması. Dolayısıyla yukarıdaki eşitliğin sağ tarafındaki ilk terim ihmal edilebilir çünkü $\\mathbf{w}$ veya $b$ ile bağımlı değildir. Maksimumu bulmak için türev alacağımız için türevi sıfır olacak birşeyi almadık. İkinci terim ise $\\frac{1}{\\sigma^2}$ katsayısı hariç hataların kareleri toplamıdır. Neyse ki çözüm $\\sigma$ varyansından bağımsızdır. Sonuç olarak, hataların karelerinin toplamını minimize etmenin, Gauss dağılımından gelen gürültü eklenmiş lineer modelin maksimum olabilirlik çıkarımına denk olduğunu gösterdik.  \n",
        "\n",
        "## Lİneer Regresyondan Derin Ağlara (Deep Networks)\n",
        "\n",
        "Şu ana kadar sadece lineer fonksiyonlala çalıştık. Sinir ağları (neural networks) çok geniş bir model ailesini kapsar ama lineer modeli sinir ağları içerisinde tanımlamak iyi bir başlangıç olacaktır. Şimdi modelimizi *katmak* (layer) gösterimi ile ifade edelim.    \n",
        "\n",
        "### Yapay Sinir Ağı Diyagramı\n",
        "\n",
        "Derin Öğrenme uygulayıcıları modelin diyagramını çizerek görselleştirmek isterler. Aşağıda lineer modeli bir sinir ağı olarak gösteriyoruz. Burada sadece bağlantılar gösteriliyor, ağırlıkların değerleri verilmiyor. Görüldüğü gibi bu durumda her girdi (input) çıktıya (output) bağlı.   \n",
        "\n",
        "![Linear regression is a single-layer neural network. ](https://d2l.ai/_images/singleneuron.svg)\n",
        "\n",
        "Gafikte, hesaplanacak bir tek nöron veya düğüm (node) olduğundan, lineer modeli tek sinir hücreli (neuron) yapay (artificial) sinir ağı olarak görürüz. Girdiler hazır verildiği için hesaplanmaz. Bu modelde her girdi çıktıya bağlı olduğundan bu dönüşüme *tam bağlantılı katman* (fully-connected layer) veya *yoğun katman* (dense layer) denir. Bu tür katmanları daha sonra ayrıntılı inceleyeceğiz.  \n",
        "\n",
        "### Biyoloji\n",
        "\n",
        "Lineer regresyon (1795) hesaplamalı yapay sinir ağlarından daha eski olsa da lineer regresyonu sinir ağlarının kronolojisine yerleştirmek doğru olmaz. Lineer modellerin yaay sinir ağlarına başlangıç için neden iyi oldğunu görmek için sibernetik/nöroeurofizyoloji uzmanı Warren McCulloch ve Walter Pitts tarafından yaay sinir ağlarının nasıl geliştirildiğine bakalım. Aşağıdaki resimde girdi terminali (dendrites), işlemci (nucleus), çıkış kablosu (axon) ve çıkış terminali (axon terminals) ve diğer nöronlara bağlantıyı sinapsis ile sağlayan bağlantılar vardır. \n",
        "\n",
        "![The real neuron](https://d2l.ai/_images/Neuron.svg)\n",
        ":label:`fig_Neuron`\n",
        "\n",
        "Diğer nöronlardan veya retina gibi algılayıcılardan gelen $x_i$ bilgisi dentrit tarafından alınır. Bu bilgilerin yani girdilerin etki dereceleri $w_i$ *sinatik ağırlıklar* sayeinde belirlenir ve $x_i w_i$ çarpımları elde edilir. Farklı algılayıcılardan gelen bu ağırlıklı girdiler nükleusta bir araya getirilerek $y = \\sum_i x_i w_i + b$ toplamı oluşturulur. Sonra işlemden geçirilmek üzere aksona gönderilir ve lineer olmayan $\\sigma(y)$ işlemi uygulanır. Buradan ya kas gibi hedefe ya da diğer bir nörona aktarılır. \n",
        "\n",
        "Bu şekilde bir çok birimin doğru biçimde bağlanarak ve doğru öğrenme algoritması ile çok karmaşı davranışlar sergileceyeceği fikri nöronların biyolojik inclenenmesi sonucu elde edilmiştir. \n",
        "\n",
        "Derin Öğrenme araştırmalarının önemli kısmı nörolojiden (neuroscience) az da olsa etkilenmiştir. *Artificial Intelligence: A Modern Approach* kitabında Stuart Russell ve Peter Norvig, uçaklar kuşlardan ilham alsa da, kuş bilimi havacılığı yönledirmemiştir demektedir. Benzer şekilde, Derin Öğrenme alanındaki gelişmeler nörolojiden çok matematik, istatistik ve bilgisayar bilimleri kaynaklıdır.  \n",
        "\n",
        "## Özet\n",
        "\n",
        "* \n",
        "* Key ingredients in a machine learning model are training data, a loss function, an optimization algorithm, and quite obviously, the model itself.\n",
        "* Vectorizing makes everything better (mostly math) and faster (mostly code).\n",
        "* Minimizing an objective function and performing maximum likelihood can mean the same thing.\n",
        "* Linear models are neural networks, too.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Assume that we have some data $x_1, \\ldots, x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized.\n",
        "    * Find a closed-form solution for the optimal value of $b$.\n",
        "    * How does this problem and its solution relate to the normal distribution?\n",
        "1. Derive the closed-form solution to the optimization problem for linear regression with squared error. To keep things simple, you can omit the bias $b$ from the problem (we can do this in principled fashion by adding one column to $X$ consisting of all ones).\n",
        "    * Write out the optimization problem in matrix and vector notation (treat all the data as a single matrix, all the target values as a single vector).\n",
        "    * Compute the gradient of the loss with respect to $w$.\n",
        "    * Find the closed form solution by setting the gradient equal to zero and solving the matrix equation.\n",
        "    * When might this be better than using stochastic gradient descent? When might this method break?\n",
        "1. Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$.\n",
        "    * Write out the negative log-likelihood of the data under the model $-\\log P(Y \\mid X)$.\n",
        "    * Can you find a closed form solution?\n",
        "    * Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "tab": "mxnet",
        "id": "4KDwmJf1zyg7",
        "colab_type": "text"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/40)"
      ]
    }
  ]
}